# ãƒ•ã‚¡ã‚¤ãƒ«å: train.py

import os
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path
from tqdm import tqdm
import numpy as np
import optuna
import json
import argparse
import matplotlib.pyplot as plt
import wandb
import gc # â˜… è¿½åŠ : ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from diffusers import DDPMScheduler, DPMSolverMultistepScheduler, EulerDiscreteScheduler
from torchmetrics.image import StructuralSimilarityIndexMeasure
from functools import partial
import torch

# --- ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å¤‰æ›´ ---
from utils import load_config, set_seed
from data_utils import Preprocessed_CT_DRR_Dataset, GridPatchDatasetWithCond
from models import DistributedUNet
from custom_models.conditioning_encoder import (
    ConditioningEncoderResNet,
    ConditioningEncoderConvNeXt,
    ConditioningEncoderEfficientNetV2
)
from custom_models.unet import DiffusionModelUNet
from torch.utils.checkpoint import checkpoint

# --- â˜… è¿½åŠ : åˆ†é›¢ã—ãŸãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
from evaluation_utils import (
    EMA,
    calculate_mae,
    generate_and_evaluate,
    create_evaluation_report
)

CONFIG = load_config()



def evaluate_epoch(device, distributed_model, scheduler, val_dataloader, trial=None, epoch=0):
    # è©•ä¾¡æ™‚ã¯EMAã®é‡ã¿ã‚’ä½¿ç”¨ã™ã‚‹
    distributed_model.ema.apply_shadow()
    distributed_model.eval()
    val_loss_epoch = []
    with torch.no_grad():
        for ct_patch, drr1, drr2, pos_3d in val_dataloader:
            ct_patch, drr1, drr2, pos_3d = ct_patch.to(device), drr1.to(device), drr2.to(device), pos_3d.to(device)
            noise = torch.randn_like(ct_patch)
            timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (ct_patch.shape[0],), device=ct_patch.device).long()
            noisy_ct = scheduler.add_noise(original_samples=ct_patch, noise=noise, timesteps=timesteps)

            # â˜… å¤‰æ›´ç‚¹: åˆ†æ•£ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–
            context = distributed_model.conditioning_encoder(drr1, drr2)
            predicted_noise = distributed_model(x=noisy_ct, timesteps=timesteps, context=context, pos_3d=pos_3d)
            loss = F.l1_loss(predicted_noise, noise)
            val_loss_epoch.append(loss.item())
    # ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å…ƒã«æˆ»ã™
    distributed_model.ema.restore()
    avg_val_loss = np.mean(val_loss_epoch)

    # --- â˜… è¿½åŠ : Optunaã®æåˆˆã‚Š(Pruning)æ©Ÿèƒ½ ---
    if trial:
        trial.report(avg_val_loss, epoch)

    return avg_val_loss


# --- å­¦ç¿’ãƒ»è©•ä¾¡é–¢æ•° ---
def train_and_evaluate(params, trial, train_paths, val_paths, loss_phase_epochs, data_config, resume_from_checkpoint=None): # noqa: E501
    start_epoch = 0 # â˜… å¤‰æ›´: é–‹å§‹ã‚¨ãƒãƒƒã‚¯ã‚’å®šç¾©
    wandb_run_id = None # â˜… è¿½åŠ : WandBå†é–‹ç”¨ã®ID
    
    # --- â˜… å¤‰æ›´: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«å¿œã˜ã¦ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‹•çš„ã«èª¿æ•´ ---
    base_batch_size = CONFIG["BATCH_SIZE"]
    model_scale = params["model_scale"]
    if model_scale == "medium":
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒç´„2å€ã«ãªã‚‹ã¨ä»®å®šã—ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’åŠåˆ†ã«
        BATCH_SIZE = max(1, base_batch_size // 2)
    elif model_scale == "large":
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒç´„4å€ã«ãªã‚‹ã¨ä»®å®šã—ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’1/4ã«
        BATCH_SIZE = max(1, base_batch_size // 4)
    else: # small
        BATCH_SIZE = base_batch_size
    
    print(f"â„¹ï¸ Model scale: '{model_scale}', Base BATCH_SIZE: {base_batch_size}, Adjusted BATCH_SIZE: {BATCH_SIZE}")
    # ---------------------------------------------------------

    # --- â˜… å¤‰æ›´: configã‹ã‚‰ãƒ‘ãƒƒãƒã‚µã‚¤ã‚ºã¨ãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚µã‚¤ã‚ºã‚’ç›´æ¥èª­ã¿è¾¼ã‚€ ---
    # ã“ã‚Œã«ã‚ˆã‚Šã€ã‚µã‚¤ã‚ºé–¢é€£ã®è¨­å®šãŒconfig.ymlã«é›†ç´„ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒæ˜ç¢ºã«ãªã‚Šã¾ã™ã€‚
    patch_size_val = CONFIG["TRAINING"]["PATCH_SIZE"]
    target_volume_size = CONFIG["DATA"]["TARGET_VOLUME_SIZE"]

    PATCH_SIZE = (patch_size_val, patch_size_val, patch_size_val)
    lr = params["learning_rate"]
    weight_decay = params["weight_decay"]
    gradient_accumulation_steps = params["gradient_accumulation_steps"]

    SAVE_PATH = f"./checkpoints/trial_{trial.number}"
    os.makedirs(SAVE_PATH, exist_ok=True)

    # --- â˜… è¿½åŠ : ãƒ‘ãƒƒãƒã‚µã‚¤ã‚ºã¨ãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚µã‚¤ã‚ºã®æ¤œè¨¼ ---
    for i in range(3):
        if PATCH_SIZE[i] > target_volume_size[i]:
            raise ValueError(f"è¨­å®šã‚¨ãƒ©ãƒ¼: PATCH_SIZE[{i}] ({PATCH_SIZE[i]}) ãŒ TARGET_VOLUME_SIZE[{i}] ({target_volume_size[i]}) ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚config.ymlã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    # --- â˜… å¤‰æ›´ç‚¹: ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—ç”¨ã®ãƒ‡ãƒã‚¤ã‚¹è¨­å®š ---
    if not torch.cuda.is_available() or torch.cuda.device_count() < 3:
        raise RuntimeError("This script requires at least 3 GPUs for model parallelism.")
    # ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒã‚¤ã‚¹ã‚’cuda:0ã¨ã™ã‚‹
    device = torch.device("cuda:0")
    print(f"Using device: {device}")

    drr_dir = Path(data_config["DRR_DIR"])
    # --- â˜… å¤‰æ›´ç‚¹: GridPatchDatasetã‚’ä½¿ç”¨ ---
    patch_overlap_ratio = params.get('patch_overlap') or 0.0 # configã«ã‚­ãƒ¼ãŒãªã„ã‹ã€å€¤ãŒNoneã®å ´åˆã«0.0ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    patch_overlap = tuple(int(p * patch_overlap_ratio) for p in PATCH_SIZE)

    train_dataset = GridPatchDatasetWithCond(
        data=train_paths, drr_dir=drr_dir, patch_size=PATCH_SIZE, patch_overlap=patch_overlap
    )
    val_dataset = GridPatchDatasetWithCond(
        data=val_paths, drr_dir=drr_dir, patch_size=PATCH_SIZE, patch_overlap=patch_overlap
    )
    vis_dataset = Preprocessed_CT_DRR_Dataset(val_paths, drr_dir, patch_size=None)  # ãƒ•ãƒ«ãƒœãƒªãƒ¥ãƒ¼ãƒ ç”¨
    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=CONFIG["NUM_WORKERS"], pin_memory=True)
    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=CONFIG["NUM_WORKERS"], pin_memory=True) # â˜… é«˜é€ŸåŒ–: pin_memory=True ã‚’è¿½åŠ 
    vis_dataloader = DataLoader(vis_dataset, batch_size=1, shuffle=False, num_workers=CONFIG["NUM_WORKERS"], pin_memory=True) # â˜… é«˜é€ŸåŒ–: pin_memory=True ã‚’è¿½åŠ 
    
    # --- â˜… å¤‰æ›´: configã‹ã‚‰ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’èª­ã¿è¾¼ã‚€ ---
    model_config = CONFIG.get("MODEL", {})
    unet_config = model_config.get("UNET", {})
    cond_enc_config = model_config.get("CONDITIONING_ENCODER", {})

    # --- â˜… å¤‰æ›´: Optunaã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦U-Netã®æ§‹æˆã‚’æ±ºå®š ---
    if model_scale == "small":
        num_channels = unet_config.get("NUM_CHANNELS", (32, 64, 128, 256))
        attention_levels = unet_config.get("ATTENTION_LEVELS", (False, True, True, True))
    elif model_scale == "medium":
        num_channels = (48, 96, 192, 384) 
        attention_levels = (False, True, True, True)
    else: # large
        num_channels = (64, 128, 256, 512)
        attention_levels = (False, True, True, True)
    
    num_res_blocks = unet_config.get("NUM_RES_BLOCKS", 2)
    encoder_name = params["encoder"]
    # ---------------------------------------------------------

    # --- â˜… å¤‰æ›´ç‚¹: ãƒ¢ãƒ‡ãƒ«ã‚’CPUä¸Šã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ– ---
    if encoder_name == 'resnet':
        conditioning_encoder = ConditioningEncoderResNet(output_dim=cond_enc_config.get("OUTPUT_DIM", 256))
    elif encoder_name == 'convnext':
        conditioning_encoder = ConditioningEncoderConvNeXt(output_dim=cond_enc_config.get("OUTPUT_DIM", 256))
    elif encoder_name == 'efficientnet':
        conditioning_encoder = ConditioningEncoderEfficientNetV2(output_dim=cond_enc_config.get("OUTPUT_DIM", 256))

    # --- â˜… å¤‰æ›´: å‹•çš„ãªãƒ¢ãƒ‡ãƒ«æ§‹æˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¸¡ã™ ---
    unet = DiffusionModelUNet(
        spatial_dims=3, in_channels=1, out_channels=1, with_conditioning=True,
        num_channels=tuple(num_channels),
        attention_levels=tuple(attention_levels),
        num_res_blocks=num_res_blocks,
        cross_attention_dim=conditioning_encoder.feature_dim
    )
    
    # --- â˜… å¤‰æ›´ç‚¹: åˆ†æ•£ãƒ¢ãƒ‡ãƒ«ãƒ©ãƒƒãƒ‘ãƒ¼ã‚’ä½œæˆã—ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åé›† ---
    distributed_model = DistributedUNet(unet, conditioning_encoder)
    # --- â˜… è¿½åŠ : EMAãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ– ---
    ema_model = EMA(distributed_model, decay=model_config.get("EMA_DECAY", 0.9999))
    distributed_model.ema = ema_model # distributed_modelã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
    # --- â˜… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„æ¡ˆ: torch.compile ã‚’é©ç”¨ ---
    # PyTorch 2.0ä»¥é™ã®JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã§ãƒ¢ãƒ‡ãƒ«ã‚’æœ€é©åŒ–ã—ã€é«˜é€ŸåŒ–ã—ã¾ã™ã€‚
    # print("ğŸš€ Applying torch.compile() for performance optimization...")
    # distributed_model = torch.compile(distributed_model, mode="reduce-overhead") # â˜… ä¿®æ­£: inductor backendã¨ã®äº’æ›æ€§å•é¡Œã®ãŸã‚ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
    model_params = list(distributed_model.parameters())
    # --- â˜… å¤‰æ›´: AMPå®‰å®šåŒ–ã®ãŸã‚ã€epsã‚’èª¿æ•´ ---
    optimizer = torch.optim.AdamW(model_params, lr=lr, weight_decay=weight_decay, betas=(0.9, 0.95), eps=1e-6)

    # --- â˜… å¤‰æ›´ç‚¹: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’configã«åŸºã¥ã„ã¦è¨­å®š ---
    scheduler_name = params.get('scheduler_name', 'linear')
    if scheduler_name == 'cosine':
        beta_schedule = "squaredcos_cap_v2"
    else: # 'linear' or default
        beta_schedule = "linear"
    scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=beta_schedule)
    ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)
    # --- â˜… å¤‰æ›´: æ··åˆç²¾åº¦(AMP)ã‚’å†åº¦æœ‰åŠ¹åŒ– ---
    scaler = torch.cuda.amp.GradScaler()

    if resume_from_checkpoint:
        checkpoint_path = Path(resume_from_checkpoint)
        # â˜… å¤‰æ›´ç‚¹: åˆ†æ•£ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
        print(f"Resuming from checkpoint: {resume_from_checkpoint}")
        distributed_model.conditioning_encoder.load_state_dict(torch.load(Path(resume_from_checkpoint) / "conditioning_encoder.pth", map_location=distributed_model.device0))
        distributed_model.time_mlp.load_state_dict(torch.load(Path(resume_from_checkpoint) / "unet_time_mlp.pth", map_location=distributed_model.device0))
        distributed_model.init_conv.load_state_dict(torch.load(Path(resume_from_checkpoint) / "unet_init_conv.pth", map_location=distributed_model.device0))
        
        # åˆ†å‰²ã•ã‚ŒãŸdown_blocksã‚’ãƒ­ãƒ¼ãƒ‰
        down_blocks_state_dict = torch.load(checkpoint_path / "unet_down_blocks.pth")
        distributed_model.down_block_0.load_state_dict({k.replace('0.', ''): v for k, v in down_blocks_state_dict.items() if k.startswith('0.')}, strict=False)
        distributed_model.down_block_1.load_state_dict({k.replace('1.', ''): v for k, v in down_blocks_state_dict.items() if k.startswith('1.')}, strict=False)
        distributed_model.down_block_2.load_state_dict({k.replace('2.', ''): v for k, v in down_blocks_state_dict.items() if k.startswith('2.')}, strict=False)
        distributed_model.down_block_3.load_state_dict({k.replace('3.', ''): v for k, v in down_blocks_state_dict.items() if k.startswith('3.')}, strict=False)

        distributed_model.mid_block1.load_state_dict(torch.load(Path(resume_from_checkpoint) / "unet_mid_block1.pth", map_location=distributed_model.device2))
        distributed_model.mid_attn.load_state_dict(torch.load(Path(resume_from_checkpoint) / "unet_mid_attn.pth", map_location=distributed_model.device2))
        distributed_model.mid_block2.load_state_dict(torch.load(Path(resume_from_checkpoint) / "unet_mid_block2.pth", map_location=distributed_model.device2))
        
        # åˆ†å‰²ã•ã‚ŒãŸup_blocksã‚’ãƒ­ãƒ¼ãƒ‰
        up_blocks_state_dict = torch.load(checkpoint_path / "unet_up_blocks.pth")
        distributed_model.up_block_0.load_state_dict({k.replace('0.', ''): v for k, v in up_blocks_state_dict.items() if k.startswith('0.')}, strict=False)
        distributed_model.up_block_1.load_state_dict({k.replace('1.', ''): v for k, v in up_blocks_state_dict.items() if k.startswith('1.')}, strict=False)
        distributed_model.up_block_2.load_state_dict({k.replace('2.', ''): v for k, v in up_blocks_state_dict.items() if k.startswith('2.')}, strict=False)
        distributed_model.up_block_3.load_state_dict({k.replace('3.', ''): v for k, v in up_blocks_state_dict.items() if k.startswith('3.')}, strict=False)
        distributed_model.out_conv.load_state_dict(torch.load(Path(resume_from_checkpoint) / "unet_out_conv.pth", map_location=distributed_model.device0))
        optimizer.load_state_dict(torch.load(checkpoint_path / "optimizer.pth", map_location=device))
        # EMAã®é‡ã¿ã‚‚ãƒ­ãƒ¼ãƒ‰
        ema_checkpoint_path = checkpoint_path / "ema_model.pth"
        if ema_checkpoint_path.exists():
            ema_model.load_state_dict(torch.load(ema_checkpoint_path, map_location=device))
        # --- â˜… å¤‰æ›´: scalerã®çŠ¶æ…‹ã‚‚ãƒ­ãƒ¼ãƒ‰ ---
        scaler.load_state_dict(torch.load(Path(resume_from_checkpoint) / "scaler.pth"))

        # â˜… å¤‰æ›´: ã‚¨ãƒãƒƒã‚¯æ•°ã¨best_val_lossã‚’ãƒ­ãƒ¼ãƒ‰
        info_path = checkpoint_path / "checkpoint_info.json"
        if info_path.exists():
            with open(info_path, 'r') as f:
                checkpoint_info = json.load(f)
            start_epoch = checkpoint_info.get('epoch', 0)
            best_val_loss = checkpoint_info.get('best_val_loss', float('inf'))
            wandb_run_id = checkpoint_info.get('wandb_run_id', None) # â˜… è¿½åŠ : WandB IDã‚’ãƒ­ãƒ¼ãƒ‰
            print(f"  -> Resuming from epoch {start_epoch} with best_val_loss {best_val_loss:.4f}")
            if wandb_run_id:
                print(f"  -> Resuming WandB run with ID: {wandb_run_id}")
        else:
            print("  -> Warning: checkpoint_info.json not found. Resuming from epoch 0.")

    best_val_loss = float('inf')
    best_epoch = -1

    # --- WandB åˆæœŸåŒ– (ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿å¾Œ) ---
    l2_end_epoch, l1_end_epoch = loss_phase_epochs
    config = {
        "encoder": encoder_name, "trial_number": trial.number, 
        "loss_schedule": f"L2(->{l2_end_epoch})_L1(->{l1_end_epoch})_SSIM",
        **params
    }
    run_name = f"trial-{trial.number}_scale-{model_scale}_enc-{encoder_name}_p-{params['patch_size']}"
    
    # â˜… å¤‰æ›´ç‚¹: å†é–‹æ™‚ã¯IDã‚’æŒ‡å®šã—ã€æ–°è¦æ™‚ã¯IDã‚’ç”Ÿæˆ
    wandb.init(
        project=CONFIG["PROJECT_NAME"], 
        config=config, 
        name=run_name, 
        id=wandb_run_id, # å†é–‹æ™‚ã¯IDã‚’æŒ‡å®šã€æ–°è¦ãªã‚‰None
        resume="must" if wandb_run_id else None # å†é–‹æ™‚ã¯"must"ã‚’æŒ‡å®š
    )
    if wandb_run_id is None:
        wandb_run_id = wandb.run.id # æ–°è¦å®Ÿè¡Œæ™‚ã«ç”Ÿæˆã•ã‚ŒãŸIDã‚’ä¿å­˜

    # --- â˜… è¿½åŠ : WandBã§ãƒ¢ãƒ‡ãƒ«ã®å‹¾é…ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç›£è¦– ---
    wandb.watch(distributed_model, log="all", log_freq=CONFIG["TRAINING"].get("VISUALIZATION_FREQ", 10) * len(train_dataloader))

    # --- â˜… å¤‰æ›´: å‹¾é…çˆ†ç™ºã®ãƒ‡ãƒãƒƒã‚°ã®ãŸã‚ã€Anomaly Detectionã‚’æœ‰åŠ¹åŒ– ---
    # ã“ã‚Œã«ã‚ˆã‚ŠNaN/Infã‚’ç”Ÿæˆã—ãŸæ“ä½œã®ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ãŒå‡ºåŠ›ã•ã‚Œã¾ã™ãŒã€å­¦ç¿’ã¯é…ããªã‚Šã¾ã™ã€‚
    # torch.autograd.set_detect_anomaly(True)

    for epoch in range(start_epoch, CONFIG["EPOCHS"]):
        distributed_model.train()
        # --- â˜… ä¿®æ­£: å…ƒã®æå¤±é–¢æ•°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã«æˆ»ã™ ---
        # ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯ã«åŸºã¥ã„ã¦æå¤±ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š
        if epoch < l2_end_epoch:
            current_loss_type = 'l2'
        elif epoch < l1_end_epoch:
            current_loss_type = 'l1'
        else:
            current_loss_type = 'l1_ssim'

        train_loss_epoch = 0.0
        
        progress_bar = tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{CONFIG['EPOCHS']} Training")
        for step, (ct_patch, drr1, drr2, pos_3d) in enumerate(progress_bar):
            ct_patch, drr1, drr2, pos_3d = ct_patch.to(device), drr1.to(device), drr2.to(device), pos_3d.to(device)
            global_step = epoch * len(train_dataloader) + step
            noise = torch.randn_like(ct_patch)
            timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (ct_patch.shape[0],), device=ct_patch.device).long()
            noisy_ct = scheduler.add_noise(original_samples=ct_patch, noise=noise, timesteps=timesteps)

            # --- â˜… å¤‰æ›´: autocastã‚’æœ‰åŠ¹åŒ– ---
            with torch.cuda.amp.autocast():
                context = distributed_model.conditioning_encoder(drr1, drr2)
                predicted_noise = checkpoint(distributed_model, noisy_ct, timesteps, context, pos_3d, use_reentrant=False)
                loss_details = {}
                
                # --- â˜… ä¿®æ­£: æå¤±é–¢æ•°ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ­ã‚¸ãƒƒã‚¯ã‚’æ­£ã—ãåæ˜  ---
                if current_loss_type == 'l1':
                    loss = F.l1_loss(predicted_noise, noise)
                elif current_loss_type == 'l2':
                    loss = F.mse_loss(predicted_noise, noise)
                else: # 'l1_ssim'
                    # --- â˜… å¤‰æ›´: L1, SSIM, ãƒ•ãƒ¼ãƒªã‚¨æå¤±ã‚’çµ„ã¿åˆã‚ã›ã‚‹ ---
                    weights = CONFIG["TRAINING"].get("LOSS_WEIGHTS", {"L1": 0.5, "SSIM": 0.5, "FOURIER": 0.0})

                    # 1. ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã§ã®L1æå¤±
                    loss_details["l1_loss"] = F.l1_loss(predicted_noise, noise)

                    # 2. çŸ¥è¦šçš„æå¤± (SSIM)
                    denoised_ct = scheduler.step(predicted_noise, timesteps, noisy_ct).pred_original_sample
                    loss_details["ssim_loss"] = 1.0 - ssim_metric(denoised_ct, ct_patch)

                    # 3. å‘¨æ³¢æ•°ç©ºé–“ã§ã®L1æå¤± (ãƒ•ãƒ¼ãƒªã‚¨æå¤±)
                    fft_predicted = torch.fft.fftn(predicted_noise, dim=[-3, -2, -1])
                    fft_target = torch.fft.fftn(noise, dim=[-3, -2, -1])
                    loss_details["fourier_loss"] = F.l1_loss(torch.abs(fft_predicted), torch.abs(fft_target))

                    # 4. å„æå¤±ã‚’é‡ã¿ä»˜ã‘ã—ã¦åˆè¨ˆ
                    loss = (weights["L1"] * loss_details["l1_loss"] + 
                            weights["SSIM"] * loss_details["ssim_loss"] + 
                            weights["FOURIER"] * loss_details["fourier_loss"])

            # å‹¾é…è“„ç©ã®ãŸã‚ã«æå¤±ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            scaled_loss = loss / gradient_accumulation_steps

            if torch.isnan(scaled_loss) or torch.isinf(scaled_loss):
                # --- â˜… å¤‰æ›´: ã‚¨ãƒ©ãƒ¼ã‚’WandBã«è¨˜éŒ²ã—ã€ç„¡é™å¤§ã®æå¤±ã‚’è¿”ã—ã¦Optunaã«å¤±æ•—ã‚’ä¼ãˆã‚‹ ---
                print(f"\nğŸ”¥ NaN or Inf loss detected at epoch {epoch+1}, step {step}. Pruning trial.")
                # --- â˜… å¤‰æ›´: ç™ºæ•£æ™‚ã®è©³ç´°ãƒ­ã‚° ---
                log_data = {
                    "train_loss": float('inf'), 
                    "val_loss": float('inf'), 
                    "epoch": epoch, 
                    "status": "pruned_nan_loss",
                    **{f"nan_loss_detail/{k}": v.item() if torch.is_tensor(v) else v for k, v in loss_details.items()}
                }
                wandb.log(log_data, step=global_step)
                wandb.summary["status"] = "pruned_nan_loss"
                wandb.summary.update({f"final_{k}": v for k, v in params.items()})
                wandb.summary.update({f"final_loss_weight_{k}": v for k, v in CONFIG["TRAINING"]["LOSS_WEIGHTS"].items()})
                # Optunaã«ã“ã®ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ãŒå¤±æ•—ã—ãŸã“ã¨ã‚’ä¼ãˆã‚‹ãŸã‚ã«å¤§ããªå€¤ã‚’è¿”ã™
                wandb.finish(exit_code=1) # çµ‚äº†ã‚³ãƒ¼ãƒ‰1ã§WandBã‚’çµ‚äº†
                raise optuna.exceptions.TrialPruned()
            
            # --- â˜… å¤‰æ›´: scalerã‚’ä½¿ã£ã¦å‹¾é…ã‚’è¨ˆç®— ---
            scaler.scale(scaled_loss).backward()

            train_loss_epoch += loss.item() * gradient_accumulation_steps # ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å…ƒã«æˆ»ã—ã¦åŠ ç®—

            # å‹¾é…è“„ç©ã‚¹ãƒ†ãƒƒãƒ—ã«é”ã—ãŸã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
            if (step + 1) % gradient_accumulation_steps == 0:
                # --- â˜… å¤‰æ›´: å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã‚’unscaleå¾Œã«è¡Œã† ---
                # 1. å‹¾é…ã‚’unscaleã™ã‚‹
                scaler.unscale_(optimizer)
                
                # 2. å‹¾é…ã‚’ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ (inf/nanãƒã‚§ãƒƒã‚¯ã‚‚å…¼ã­ã‚‹)
                grad_norm = torch.nn.utils.clip_grad_norm_(model_params, CONFIG["MAX_GRAD_NORM"])
                wandb.log({"details/grad_norm": grad_norm.item()}, step=global_step)

                # 3. optimizerã‚’æ›´æ–° (scalerãŒinf/nanã‚’æ¤œçŸ¥ã—ãŸã‚‰ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹)
                scaler.step(optimizer)
                
                # 4. scalerã‚’æ›´æ–°
                scaler.update()
                ema_model.update()
                optimizer.zero_grad()
            
            # --- â˜… è¿½åŠ : ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®è©³ç´°ãªæå¤±ã‚’ãƒ­ã‚° ---
            if (step + 1) % gradient_accumulation_steps == 0:
                log_step_data = {"details/step_loss": loss.item()}
                if loss_details:
                    log_step_data.update({f"details/loss_{k}": v.item() for k, v in loss_details.items()})
                wandb.log(log_step_data, step=global_step)
            
            # --- â˜… è¿½åŠ : ãƒ¡ãƒ¢ãƒªè§£æ”¾ ---
            # ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ä¸è¦ã«ãªã£ãŸãƒ†ãƒ³ã‚½ãƒ«ã‚’æ˜ç¤ºçš„ã«å‰Šé™¤
            del ct_patch, drr1, drr2, pos_3d, noise, noisy_ct, context, predicted_noise, loss, scaled_loss
            if 'denoised_ct' in locals(): del denoised_ct
            if 'fft_predicted' in locals(): del fft_predicted, fft_target
        
        avg_val_loss = evaluate_epoch(device, distributed_model, scheduler, val_dataloader, trial, epoch)
        
        avg_train_loss = train_loss_epoch / len(train_dataloader)
        print(f"Epoch {epoch+1}/{CONFIG['EPOCHS']} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
        # --- â˜… å¤‰æ›´: ã‚¨ãƒãƒƒã‚¯ã”ã¨ã®ãƒ­ã‚°ã«å­¦ç¿’ç‡ã‚’è¿½åŠ  ---
        wandb.log({"train_loss": avg_train_loss, "val_loss": avg_val_loss, "epoch": epoch,
                   "learning_rate": optimizer.param_groups[0]['lr']}, step=global_step)

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_epoch = epoch + 1
            print(f"  âœ¨ New best val loss: {best_val_loss:.4f} at epoch {best_epoch}. Saving model to {SAVE_PATH}")
            # â˜… å¤‰æ›´ç‚¹: åˆ†æ•£ãƒ¢ãƒ‡ãƒ«ã®å„ãƒ‘ãƒ¼ãƒ„ã‚’ä¿å­˜
            torch.save(distributed_model.conditioning_encoder.state_dict(), Path(SAVE_PATH) / "conditioning_encoder.pth")
            torch.save(distributed_model.time_mlp.state_dict(), Path(SAVE_PATH) / "unet_time_mlp.pth")
            torch.save(distributed_model.init_conv.state_dict(), Path(SAVE_PATH) / "unet_init_conv.pth")
            # åˆ†å‰²ã•ã‚ŒãŸdown_blocksã‚’çµåˆã—ã¦ä¿å­˜
            down_blocks_state_dict = {**{'0.'+k: v for k,v in distributed_model.down_block_0.state_dict().items()}, **{'1.'+k: v for k,v in distributed_model.down_block_1.state_dict().items()}, **{'2.'+k: v for k,v in distributed_model.down_block_2.state_dict().items()}, **{'3.'+k: v for k,v in distributed_model.down_block_3.state_dict().items()}}
            torch.save(down_blocks_state_dict, Path(SAVE_PATH) / "unet_down_blocks.pth")
            torch.save(distributed_model.mid_block1.state_dict(), Path(SAVE_PATH) / "unet_mid_block1.pth")
            torch.save(distributed_model.mid_attn.state_dict(), Path(SAVE_PATH) / "unet_mid_attn.pth")
            torch.save(distributed_model.mid_block2.state_dict(), Path(SAVE_PATH) / "unet_mid_block2.pth")
            up_blocks_state_dict = {**{'0.'+k: v for k,v in distributed_model.up_block_0.state_dict().items()}, **{'1.'+k: v for k,v in distributed_model.up_block_1.state_dict().items()}, **{'2.'+k: v for k,v in distributed_model.up_block_2.state_dict().items()}, **{'3.'+k: v for k,v in distributed_model.up_block_3.state_dict().items()}} # åˆ†å‰²ã•ã‚ŒãŸup_blocksã‚’çµåˆã—ã¦ä¿å­˜
            torch.save(up_blocks_state_dict, Path(SAVE_PATH) / "unet_up_blocks.pth") # ä¿®æ­£æ¸ˆã¿
            torch.save(distributed_model.out_conv.state_dict(), Path(SAVE_PATH) / "unet_out_conv.pth")
            torch.save(ema_model.state_dict(), Path(SAVE_PATH) / "ema_model.pth") # â˜… è¿½åŠ : EMAã®é‡ã¿ã‚’ä¿å­˜
            torch.save(optimizer.state_dict(), Path(SAVE_PATH) / "optimizer.pth")
            torch.save(scaler.state_dict(), Path(SAVE_PATH) / "scaler.pth") # â˜… å¤‰æ›´: æ··åˆç²¾åº¦ã‚’æœ‰åŠ¹åŒ–

            # â˜… å¤‰æ›´: ã‚¨ãƒãƒƒã‚¯æ•°ã¨æ¤œè¨¼ãƒ­ã‚¹ã‚’ä¿å­˜
            checkpoint_info = {
                'epoch': best_epoch,
                'best_val_loss': best_val_loss,
                'wandb_run_id': wandb_run_id # â˜… è¿½åŠ : WandBã®Run IDã‚’ä¿å­˜
            }
            with open(Path(SAVE_PATH) / "checkpoint_info.json", 'w') as f:
                json.dump(checkpoint_info, f, indent=4)

        # --- â˜… è¿½åŠ : å®šæœŸçš„ãªå¯è¦–åŒ–ãƒ—ãƒ­ã‚»ã‚¹ ---
        visualization_freq = CONFIG["TRAINING"].get("VISUALIZATION_FREQ", 0)
        # visualization_freqãŒ0ã‚ˆã‚Šå¤§ãã„å ´åˆã€ãã®é »åº¦ã§å®Ÿè¡Œ
        # ã¾ãŸã¯ã€æœ€çµ‚ã‚¨ãƒãƒƒã‚¯ã®å ´åˆã«å®Ÿè¡Œ
        if (visualization_freq > 0 and (epoch + 1) % visualization_freq == 0) or ((epoch + 1) == CONFIG["EPOCHS"]):
            print(f"--- ğŸ–¼ï¸ Running visualization for epoch {epoch + 1} ---")
            # --- â˜… å¤‰æ›´: å¯è¦–åŒ–ç›´å‰ã«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ ---
            try:
                fixed_vis_data = next(iter(vis_dataloader))
                vis_ct_full, vis_drr1, vis_drr2, vis_pos_3d = fixed_vis_data
                generate_and_evaluate(
                    device,
                    {"encoder": encoder_name, **params},
                    "dpm_solver", # å¯è¦–åŒ–ã«ã¯é«˜é€Ÿãªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ä½¿ç”¨
                    vis_ct_full,
                    vis_drr1,
                    vis_drr2,
                    vis_pos_3d,
                    epoch + 1, # ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯ç•ªå·ã‚’æ¸¡ã™
                    trial.number, SAVE_PATH,
                    model_for_inference=distributed_model
                )
            except Exception as e:
                print(f"âŒ An error occurred during periodic visualization at epoch {epoch + 1}: {e}")
            finally:
                # --- â˜… è¿½åŠ : å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ã®ãƒ¡ãƒ¢ãƒªè§£æ”¾ ---
                if 'fixed_vis_data' in locals(): del fixed_vis_data
                if 'vis_ct_full' in locals(): del vis_ct_full, vis_drr1, vis_drr2, vis_pos_3d
    
        # --- â˜… è¿½åŠ : ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã«ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ---
        gc.collect()
        torch.cuda.empty_cache()
    
    if best_epoch != -1:
        print(f"âœ¨ Generating final visualization for Trial {trial.number} with best weights (from epoch {best_epoch})...")
        try:
            fixed_vis_data = next(iter(vis_dataloader))
            vis_ct_full, vis_drr1, vis_drr2, vis_pos_3d = fixed_vis_data

            generate_and_evaluate(
                device,
                {"encoder": encoder_name, **params},
                "", # defaultã®DDPMã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ä½¿ç”¨
                vis_ct_full,
                vis_drr1,
                vis_drr2,
                vis_pos_3d,
                best_epoch, 
                trial.number, SAVE_PATH,
                model_for_inference=distributed_model, # â˜… ä¿®æ­£: è¾æ›¸ã§ã¯ãªãã€DistributedUNetã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç›´æ¥æ¸¡ã™
                inference_steps=200 # æœ€çµ‚è©•ä¾¡ã§ã¯ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å¢—ã‚„ã—ã¦é«˜å“è³ªåŒ–
           )
        except Exception as e:
            print(f"An error occurred during final visualization: {e}")
    else:
        print("No best model found to visualize for this trial.")

    wandb.finish()
    
    return best_val_loss

# --- mainé–¢æ•° (ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã¨ãƒ‡ãƒ¼ã‚¿åŒæœŸã‚’ä¿®æ­£) ---
def main(args):
    # --- cuDNNã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ã®è¨­å®š ---
    # torch.backends.cudnn.enabled = False
    torch.backends.cudnn.benchmark = False
    print("â„¹ï¸ Set torch.backends.cudnn.benchmark = False to avoid potential cuDNN errors.")

    # --- â˜… å¤‰æ›´ç‚¹: configã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®šã‚’èª­ã¿è¾¼ã‚€ ---
    data_config = CONFIG["DATA"]
    pt_dir = Path(data_config["PT_DATA_DIR"])
    drr_dir = Path(data_config["DRR_DIR"])

    # --- â˜… å¤‰æ›´ç‚¹: ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ ---
    cache_file = Path("./verified_paths.json")

    if cache_file.exists():
        print(f"âœ… Loading verified data paths from cache: {cache_file}")
        with open(cache_file, 'r') as f:
            cached_data = json.load(f)
        train_paths = [Path(p) for p in cached_data['train']]
        val_paths = [Path(p) for p in cached_data['val']]
        print(f"  -> Found {len(train_paths)} training paths and {len(val_paths)} validation paths.")
    else:
        print(f"ğŸ” No cache file found. Verifying all data pairs (this will run only once)...")
        print(f"   Searching for all preprocessed tensor files in: {pt_dir}")
        all_pt_files = sorted(list(pt_dir.glob("*.pt")))
        
        verified_file_paths = []
        for ct_path in tqdm(all_pt_files, desc="Verifying data pairs"):
            drr_subdir_name = ct_path.stem
            drr_ap_path = drr_dir / drr_subdir_name / "AP.pt"
            drr_lat_path = drr_dir / drr_subdir_name / "LAT.pt"
            if drr_ap_path.exists() and drr_lat_path.exists():
                verified_file_paths.append(ct_path)
        
        if not verified_file_paths:
            print(f"ã‚¨ãƒ©ãƒ¼: æœ‰åŠ¹ãªCTã¨DRRã®ãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
            return
        
        print(f"ğŸ” Found {len(verified_file_paths)} verified data pairs.")
        train_paths, val_paths = train_test_split(
            verified_file_paths, test_size=CONFIG["VALIDATION_SPLIT"], random_state=CONFIG["SEED"]
        )
        with open(cache_file, 'w') as f:
            json.dump({'train': [str(p) for p in train_paths], 'val': [str(p) for p in val_paths]}, f, indent=4)
        print(f"ğŸ’¾ Saved verified paths to cache file: {cache_file}")

    # --- â˜… è¿½åŠ : config.ymlã®DATA_RATIOã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‰Šæ¸› ---
    data_ratio = CONFIG["DATA"]["DATA_RATIO"]
    if data_ratio < 1.0:
        print(f"âš ï¸ Using only {data_ratio * 100:.1f}% of the dataset for training and validation based on DATA_RATIO in config.")
        
        num_train = int(len(train_paths) * data_ratio)
        num_val = int(len(val_paths) * data_ratio)
        
        train_paths = train_paths[:num_train]
        val_paths = val_paths[:num_val]
        
        print(f"  -> Reduced to {len(train_paths)} training samples and {len(val_paths)} validation samples.")

    # Optunaå­¦ç¿’ãƒ«ãƒ¼ãƒ—
    if args.evaluate_only:
        # --- è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ ---
        if not args.checkpoint_dir:
            raise ValueError("--evaluate_only mode requires --checkpoint_dir to be specified.")
        
        print(f"\n--- Running in EVALUATION-ONLY mode for checkpoint: {args.checkpoint_dir} ---")
        set_seed(CONFIG["SEED"])
        
        encoder_name = CONFIG["TRAINING"]["ENCODER"]
        # 1. ãƒ‡ãƒã‚¤ã‚¹ã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™
        device = torch.device("cuda:0")
        vis_dataset = Preprocessed_CT_DRR_Dataset(val_paths, drr_dir, patch_size=None)
        vis_dataloader = DataLoader(vis_dataset, batch_size=1, shuffle=False, num_workers=CONFIG["NUM_WORKERS"], pin_memory=True)
        fixed_vis_data = next(iter(vis_dataloader))
        vis_ct_full, vis_drr1, vis_drr2, vis_pos_3d = fixed_vis_data

        # è©•ä¾¡æ™‚ã¯config.ymlã®TRAININGã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        params = {
            "patch_size": CONFIG["TRAINING"]["PATCH_SIZE"],
            "patch_overlap": CONFIG["TRAINING"].get("patch_overlap", 0.5),
            "blend_mode": CONFIG["TRAINING"].get("blend_mode", "cosine"),
            # ä»¥ä¸‹ã¯generate_and_evaluateã§ç›´æ¥ã¯ä½¿ã‚ã‚Œãªã„ãŒã€å¿µã®ãŸã‚è¨­å®š
            "learning_rate": CONFIG["TRAINING"]["LEARNING_RATE"],
            "weight_decay": CONFIG["TRAINING"]["WEIGHT_DECAY"],
        }
        # 2. ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
        if encoder_name == 'resnet':
            conditioning_encoder = ConditioningEncoderResNet(output_dim=256)
        elif encoder_name == 'convnext':
            conditioning_encoder = ConditioningEncoderConvNeXt(output_dim=256)
        else: # efficientnet
            conditioning_encoder = ConditioningEncoderEfficientNetV2(output_dim=256)

        unet_full = DiffusionModelUNet(
            spatial_dims=3, in_channels=1, out_channels=1, with_conditioning=True,
            num_channels=(32, 64, 128, 256), attention_levels=(False, True, True, True),
            num_res_blocks=2, cross_attention_dim=conditioning_encoder.feature_dim
        )
        distributed_model = DistributedUNet(unet_full, conditioning_encoder)

        # 3. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
        print(f"Loading model weights from {args.checkpoint_dir}...")
        checkpoint_path = Path(args.checkpoint_dir)
        distributed_model.conditioning_encoder.load_state_dict(torch.load(checkpoint_path / "conditioning_encoder.pth", map_location=distributed_model.device0))
        distributed_model.time_mlp.load_state_dict(torch.load(checkpoint_path / "unet_time_mlp.pth", map_location=distributed_model.device0))
        distributed_model.init_conv.load_state_dict(torch.load(checkpoint_path / "unet_init_conv.pth", map_location=distributed_model.device0))
        down_blocks_state_dict = torch.load(checkpoint_path / "unet_down_blocks.pth")
        distributed_model.down_block_0.load_state_dict({k.replace('0.', ''): v for k, v in down_blocks_state_dict.items() if k.startswith('0.')}, strict=False)
        distributed_model.down_block_1.load_state_dict({k.replace('1.', ''): v for k, v in down_blocks_state_dict.items() if k.startswith('1.')}, strict=False)
        distributed_model.down_block_2.load_state_dict({k.replace('2.', ''): v for k, v in down_blocks_state_dict.items() if k.startswith('2.')}, strict=False)
        distributed_model.down_block_3.load_state_dict({k.replace('3.', ''): v for k, v in down_blocks_state_dict.items() if k.startswith('3.')}, strict=False)
        distributed_model.mid_block1.load_state_dict(torch.load(checkpoint_path / "unet_mid_block1.pth", map_location=distributed_model.device2))
        distributed_model.mid_attn.load_state_dict(torch.load(checkpoint_path / "unet_mid_attn.pth", map_location=distributed_model.device2))
        distributed_model.mid_block2.load_state_dict(torch.load(checkpoint_path / "unet_mid_block2.pth", map_location=distributed_model.device2))
        up_blocks_state_dict = torch.load(checkpoint_path / "unet_up_blocks.pth")
        distributed_model.up_block_0.load_state_dict({k.replace('0.', ''): v for k, v in up_blocks_state_dict.items() if k.startswith('0.')}, strict=False)
        distributed_model.up_block_1.load_state_dict({k.replace('1.', ''): v for k, v in up_blocks_state_dict.items() if k.startswith('1.')}, strict=False)
        distributed_model.up_block_2.load_state_dict({k.replace('2.', ''): v for k, v in up_blocks_state_dict.items() if k.startswith('2.')}, strict=False)
        distributed_model.up_block_3.load_state_dict({k.replace('3.', ''): v for k, v in up_blocks_state_dict.items() if k.startswith('3.')}, strict=False)
        distributed_model.out_conv.load_state_dict(torch.load(checkpoint_path / "unet_out_conv.pth", map_location=distributed_model.device0))
        # EMAãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        ema_model = EMA(distributed_model, decay=0.9999)
        ema_checkpoint_path = checkpoint_path / "ema_model.pth"
        if ema_checkpoint_path.exists():
            ema_model.load_state_dict(torch.load(ema_checkpoint_path, map_location=device))
        
        with open(checkpoint_path / "checkpoint_info.json", 'r') as f:
            best_epoch = json.load(f).get('epoch', 'N/A')

        # 4. è©•ä¾¡é–¢æ•°ã‚’å®Ÿè¡Œ (trial_numberã¯0ã§å›ºå®š)
        generate_and_evaluate(device, {"encoder": encoder_name, **params}, "", vis_ct_full, vis_drr1, vis_drr2, vis_pos_3d, best_epoch, 0, str(checkpoint_path), model_for_inference=distributed_model)
        print("\n--- Evaluation Finished ---")
    else:
        # --- â˜… å¤‰æ›´: Optunaã«ã‚ˆã‚‹å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ ---
        set_seed(CONFIG["SEED"])
        print(f"--- Running Optuna hyperparameter search (Seed: {CONFIG['SEED']}) ---")

        # å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’configã‹ã‚‰èª­ã¿è¾¼ã‚€
        loss_phase_epochs = CONFIG["TRAINING"]["LOSS_PHASE_EPOCHS"]
        optuna_params = CONFIG["OPTUNA"]["PARAMS"]

        def objective(trial):
            # Optunaã‹ã‚‰ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ææ¡ˆ
            params = {
                "encoder": trial.suggest_categorical("encoder", optuna_params["encoder"]),
                "model_scale": trial.suggest_categorical("model_scale", optuna_params["model_scale"]),
                "patch_size": trial.suggest_categorical("patch_size", optuna_params["patch_size"]),
                # --- â˜… ä¿®æ­£: Optunaã®éæ¨å¥¨APIã‚’æ–°ã—ã„APIã«å¤‰æ›´ ---
                "learning_rate": trial.suggest_float("learning_rate", *optuna_params["learning_rate"], log=True),
                "weight_decay": trial.suggest_float("weight_decay", *optuna_params["weight_decay"], log=True),
                "gradient_accumulation_steps": trial.suggest_categorical("gradient_accumulation_steps", optuna_params["gradient_accumulation_steps"]),
                "patch_overlap": CONFIG["TRAINING"]["patch_overlap"], # â˜… å¤‰æ›´: configã‹ã‚‰å›ºå®šå€¤ã‚’èª­ã¿è¾¼ã‚€
                "blend_mode": trial.suggest_categorical("blend_mode", optuna_params["blend_mode"]),
                "scheduler_name": trial.suggest_categorical("scheduler_name", optuna_params["scheduler_name"]),
            }
            
            # æå¤±ã®é‡ã¿ã‚’ææ¡ˆã—ã€åˆè¨ˆãŒ1ã«ãªã‚‹ã‚ˆã†ã«æ­£è¦åŒ–
            l1 = trial.suggest_float("loss_weight_l1", *optuna_params["loss_weight_l1"])
            ssim = trial.suggest_float("loss_weight_ssim", *optuna_params["loss_weight_ssim"])
            fourier = trial.suggest_float("loss_weight_fourier", *optuna_params["loss_weight_fourier"])
            total_weight = l1 + ssim + fourier
            CONFIG["TRAINING"]["LOSS_WEIGHTS"] = {
                "L1": l1 / total_weight,
                "SSIM": ssim / total_weight,
                "FOURIER": fourier / total_weight,
            }

            # --- â˜… å¤‰æ›´: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’è¿½åŠ  ---
            try:
                # train_and_evaluateã¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å†é–‹ã‚’ã‚µãƒãƒ¼ãƒˆ
                # Optunaã¯é€šå¸¸ã€å„ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã‚’ç‹¬ç«‹ã—ã¦å®Ÿè¡Œã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯resume_from_checkpoint=Noneã¨ã™ã‚‹
                # ã‚‚ã—ãƒˆãƒ©ã‚¤ã‚¢ãƒ«è‡ªä½“ã®å†é–‹ã‚’å®Ÿè£…ã—ãŸã„å ´åˆã¯ã€ã‚ˆã‚Šé«˜åº¦ãªçŠ¶æ…‹ç®¡ç†ãŒå¿…è¦
                result = train_and_evaluate(params, trial, train_paths, val_paths, loss_phase_epochs, data_config, resume_from_checkpoint=None)
                
                # --- â˜… è¿½åŠ : æåˆˆã‚Šã®ãƒã‚§ãƒƒã‚¯ ---
                if trial.should_prune():
                    raise optuna.exceptions.TrialPruned()

                return result
            except optuna.exceptions.TrialPruned:
                print(f"ğŸƒ Trial {trial.number} pruned. ğŸƒ")
                return float('inf')
            except Exception as e:
                print(f"ğŸ”¥ğŸ”¥ğŸ”¥ Trial {trial.number} failed with an exception: {e} ğŸ”¥ğŸ”¥ğŸ”¥")
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã‚’FAILã¨ã—ã¦è¨˜éŒ²ã—ã€æ¬¡ã®ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã«é€²ã‚€
                # WandBã«ã‚‚å¤±æ•—ã‚’è¨˜éŒ²
                # --- â˜… å¤‰æ›´: OOMã‚¨ãƒ©ãƒ¼ã‚’æ˜ç¤ºçš„ã«ãƒ­ã‚° ---
                error_msg = str(e)
                is_oom = "out of memory" in error_msg.lower()
                run = wandb.init(project=CONFIG["PROJECT_NAME"], name=f"trial-{trial.number}-{'OOM' if is_oom else 'FAILED'}", config=params, reinit=True)
                wandb.log({"status": "oom" if is_oom else "failed", "error_message": error_msg})
                # --- â˜… è¿½åŠ : å¤±æ•—æ™‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒãƒªãƒ¼ã«è¨˜éŒ² ---
                wandb.summary["status"] = "oom" if is_oom else "failed"
                wandb.summary.update({f"failed_param_{k}": v for k, v in trial.params.items()})
                run.finish(exit_code=1)
                return float('inf') # Optunaã«å¤§ããªæå¤±å€¤ã‚’è¿”ã—ã¦å¤±æ•—ã‚’ä¼ãˆã‚‹

        study = optuna.create_study(direction="minimize")
        study.optimize(objective, n_trials=CONFIG["OPTUNA"]["N_TRIALS"])
        print("\n--- Optuna Search Finished ---")
        print(f"Best trial: {study.best_trial.value}")
        print(f"Best params: {study.best_trial.params}")

if __name__ == '__main__':
    set_seed(CONFIG["SEED"])
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰resume_from_checkpointã®ã¿ã‚’å–å¾—ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´
    parser = argparse.ArgumentParser(description="Train or evaluate a conditioned diffusion model.")
    parser.add_argument("--trial_number", type=int, default=0,
                        help="The trial number for this run, used for naming checkpoints and outputs.")
    parser.add_argument("--checkpoint_dir", type=str, default=None,
                        help="Path to a checkpoint directory to resume training from or for evaluation.")
    parser.add_argument("--evaluate_only", action="store_true",
                        help="If specified, skips training and runs evaluation on the model in --checkpoint_dir.")
    cli_args = parser.parse_args()
    main(cli_args)