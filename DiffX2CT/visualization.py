# ãƒ•ã‚¡ã‚¤ãƒ«å: visualization.py

import torch
from pathlib import Path
import argparse
from tqdm import tqdm
import wandb
import os
import numpy as np

# train.pyã‹ã‚‰å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™
# ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯train.pyã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¦ãã ã•ã„
try:
    # --- â˜… å¤‰æ›´: train.py ã¨ evaluation_utils.py ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
    from train import (
        Preprocessed_CT_DRR_Dataset,
        CONFIG, set_seed,
        DistributedUNet,
        ConditioningEncoderResNet,
        ConditioningEncoderConvNeXt,
        ConditioningEncoderEfficientNetV2,
        DiffusionModelUNet
    )
    from evaluation_utils import (
        generate_and_evaluate,
        create_evaluation_report,
        calculate_mae
    )
except ImportError as e:
    print("ã‚¨ãƒ©ãƒ¼: train.pyã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    print("ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (visualization.py) ãŒ train.py ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    print(f"è©³ç´°: {e}")
    exit()

def test_visualization(trial_number, checkpoint_dir, encoder_name, gpu_mode, evaluate_npy_path=None):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦visualize_and_save_mprã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚
    """
    print("--- Visualization Test Script ---")

    # 1. ãƒ‡ãƒã‚¤ã‚¹ã®æº–å‚™
    # â˜… å¤‰æ›´ç‚¹: 3GPUãƒã‚§ãƒƒã‚¯ã‚’ç·©å’Œã—ã€1GPUä»¥ä¸Šã‚ã‚Œã°å‹•ä½œã™ã‚‹ã‚ˆã†ã«ã™ã‚‹
    if gpu_mode == 'multi' and torch.cuda.device_count() < 3:
        raise RuntimeError("Multi GPU mode requires at least 3 GPUs.")
    elif not torch.cuda.is_available():
        raise RuntimeError("This script requires at least one GPU for single or multi mode.")

    device = torch.device("cuda:0")
    print(f"Using GPU mode: {gpu_mode.upper()}")
    
    # ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®šã—ã¦å†ç¾æ€§ã‚’ç¢ºä¿
    set_seed(CONFIG["SEED"])

    # --- WandB åˆæœŸåŒ– ---
    run_name = f"vis-trial-{trial_number}_enc-{encoder_name}_mode-{gpu_mode}"
    wandb.init(project=CONFIG["PROJECT_NAME"], name=run_name, reinit=True)
    print(f"WandB run initialized for visualization: {run_name}")

    # 2. ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    #    æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ•ãƒ«ãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚’1ã¤å–å¾—ã—ã¾ã™ã€‚
    data_config = CONFIG["DATA"]
    drr_dir = Path(data_config["DRR_DIR"])
    pt_dir = Path(data_config["PT_DATA_DIR"])
    
    # train.pyã¨åŒæ§˜ã«ã€æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãƒšã‚¢ã‚’1ã¤è¦‹ã¤ã‘ã¾ã™
    all_pt_files = sorted(list(pt_dir.glob("*.pt"))) # rglobã‹ã‚‰globã«å¤‰æ›´
    verified_file_paths = []
    for ct_path in tqdm(all_pt_files, desc="Verifying data pairs for test"):
        drr_subdir_name = ct_path.stem
        drr_ap_path = drr_dir / drr_subdir_name / "AP.pt"
        drr_lat_path = drr_dir / drr_subdir_name / "LAT.pt"
        if drr_ap_path.exists() and drr_lat_path.exists():
            verified_file_paths.append(ct_path)
            # evaluate_npyãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€å¯¾å¿œã™ã‚‹æ­£è§£CTã‚’è¦‹ã¤ã‘ã‚‹
            if evaluate_npy_path and Path(evaluate_npy_path).stem.startswith(f"generated_ct_trial_{trial_number}"):
                # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰IDã‚’æ¨æ¸¬ (ä¾‹: generated_ct_trial_0_epoch_999_HU -> 0)
                # ã“ã“ã§ã¯æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã§ä»£ç”¨ã—ã¾ã™ãŒã€ã‚ˆã‚Šæ­£ç¢ºãªãƒãƒƒãƒãƒ³ã‚°ãŒå¿…è¦ã§ã™
                print(f"Found corresponding ground truth for {evaluate_npy_path}: {ct_path.name}")
                break

    if not verified_file_paths:
        print("ã‚¨ãƒ©ãƒ¼: ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨ã§ãã‚‹æœ‰åŠ¹ãªCT/DRRãƒ‡ãƒ¼ã‚¿ãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # ãƒ•ãƒ«ãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚’å–å¾—ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    vis_dataset = Preprocessed_CT_DRR_Dataset(verified_file_paths, drr_dir, patch_size=None)
    if len(vis_dataset) == 0:
        print("ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç©ºã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return
        
    # æœ€åˆã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ ã—ã¾ã™
    ct_full, drr1, drr2, pos_3d = vis_dataset[0]
    ct_full, drr1, drr2, pos_3d = ct_full.unsqueeze(0), drr1.unsqueeze(0), drr2.unsqueeze(0), pos_3d.unsqueeze(0)
    print(f"Loaded data shapes: CT={ct_full.shape}, DRR1={drr1.shape}, DRR2={drr2.shape}")

    # --- â˜… è¿½åŠ : NPYè©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ ---
    if evaluate_npy_path:
        print(f"\n--- Running in NPY-EVALUATION mode for: {evaluate_npy_path} ---")
        try:
            generated_ct_np = np.load(evaluate_npy_path)
            print(f"  Loaded generated CT from .npy file. Shape: {generated_ct_np.shape}")

            # æ­£è§£CTã‚’HUå€¤ã«å¤‰æ›
            min_hu, max_hu = -1024, 1500
            ground_truth_hu_np = ct_full.squeeze().cpu().numpy() * (max_hu - min_hu) + min_hu

            # è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ (train.pyã®generate_and_evaluateã‹ã‚‰ä¸€éƒ¨ã‚’æŠœç²‹ãƒ»æ”¹é€ )
            from torchmetrics.image import StructuralSimilarityIndexMeasure
            import matplotlib.pyplot as plt
            # --- â˜… ä¿®æ­£: psnré–¢æ•°ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
            from skimage.metrics import peak_signal_noise_ratio as psnr

            print("  ğŸ“Š Calculating quality metrics...")
            data_range = max_hu - min_hu
            ssim_score = StructuralSimilarityIndexMeasure(data_range=data_range)(torch.from_numpy(generated_ct_np).unsqueeze(0).unsqueeze(0), torch.from_numpy(ground_truth_hu_np).unsqueeze(0).unsqueeze(0)).item()
            psnr_score = psnr(ground_truth_hu_np, generated_ct_np, data_range=data_range)
            mae_score = calculate_mae(ground_truth_hu_np, generated_ct_np)
            print(f"  -> SSIM={ssim_score:.4f}, PSNR={psnr_score:.2f} dB, MAE={mae_score:.2f} HU")

            # â˜… å¤‰æ›´: è©³ç´°ãªè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
            print("  ğŸ–¼ï¸ Creating detailed evaluation report...")
            fig = create_evaluation_report(
                generated_hu_np=generated_ct_np,
                ground_truth_hu_np=ground_truth_hu_np,
                ssim_score=ssim_score,
                psnr_score=psnr_score,
                mae_score=mae_score,
                title_prefix=f'Evaluation Report for {Path(evaluate_npy_path).name}'
            )

            save_path = Path(checkpoint_dir) / "evaluation" / f"evaluation_report_{Path(evaluate_npy_path).stem}.png"
            save_path.parent.mkdir(exist_ok=True, parents=True)
            plt.savefig(save_path, facecolor='black')
            print(f"âœ… Evaluation report for .npy saved to: {save_path}")
            wandb.log({"NPY_Evaluation_Report": wandb.Image(fig)})
            plt.close(fig)
        except Exception as e:
            print(f"âŒ Error during .npy evaluation: {e}")
        return # NPYè©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ã“ã“ã§çµ‚äº†

    # 3. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æº–å‚™
    #    ã“ã®ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã§ä½¿ã‚ã‚ŒãŸã§ã‚ã‚ã†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨¡å€£ã—ã¾ã™ã€‚
    #    visualize_and_save_mprãŒå¿…è¦ã¨ã™ã‚‹ã®ã¯ä¸»ã«'encoder'ã‚­ãƒ¼ã§ã™ã€‚
    params = {
        "encoder": encoder_name,
        "patch_size": CONFIG["TRAINING"]["PATCH_SIZE"], # train.pyã®SlidingWindowInfererã§å¿…è¦
        "learning_rate": 1e-4, # train.pyã®paramsã‚­ãƒ¼ã«åˆã‚ã›ã‚‹
        "weight_decay": 1e-5, # åŒä¸Š
        "gradient_accumulation_steps": 1, # åŒä¸Š
    }
    
    # 4. GPUãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™
    print(f"\nPreparing model for {gpu_mode.upper()} GPU mode...")
    model_for_inference = None

    if gpu_mode == 'multi':
        print("  Instantiating and loading DistributedUNet for 3 GPUs...")
        if encoder_name == 'resnet':
            conditioning_encoder = ConditioningEncoderResNet(output_dim=256)
        elif encoder_name == 'convnext':
            conditioning_encoder = ConditioningEncoderConvNeXt(output_dim=256)
        else: # efficientnet
            conditioning_encoder = ConditioningEncoderEfficientNetV2(output_dim=256)
        
        # â˜… ä¿®æ­£: ãƒ€ãƒŸãƒ¼ã§ã¯ãªãã€å®Œå…¨ãªU-Netãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹
        unet_full = DiffusionModelUNet(
            spatial_dims=3, in_channels=1, out_channels=1, with_conditioning=True,
            num_channels=(32, 64, 128, 256), attention_levels=(False, True, True, True),
            num_res_blocks=2, cross_attention_dim=conditioning_encoder.feature_dim
        )
        
        distributed_model = DistributedUNet(unet_full, conditioning_encoder)
        
        # åˆ†æ•£ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
        distributed_model.conditioning_encoder.load_state_dict(torch.load(Path(checkpoint_dir) / "conditioning_encoder.pth", map_location=distributed_model.device0))
        distributed_model.time_mlp.load_state_dict(torch.load(Path(checkpoint_dir) / "unet_time_mlp.pth", map_location=distributed_model.device0))
        distributed_model.init_conv.load_state_dict(torch.load(Path(checkpoint_dir) / "unet_init_conv.pth", map_location=distributed_model.device0))
        down_blocks_state_dict = torch.load(Path(checkpoint_dir) / "unet_down_blocks.pth")
        distributed_model.down_block_0.load_state_dict({k.replace('0.', ''): v for k, v in down_blocks_state_dict.items() if k.startswith('0.')}, strict=False)
        distributed_model.down_block_1.load_state_dict({k.replace('1.', ''): v for k, v in down_blocks_state_dict.items() if k.startswith('1.')}, strict=False)
        distributed_model.down_block_2.load_state_dict({k.replace('2.', ''): v for k, v in down_blocks_state_dict.items() if k.startswith('2.')}, strict=False)
        distributed_model.down_block_3.load_state_dict({k.replace('3.', ''): v for k, v in down_blocks_state_dict.items() if k.startswith('3.')}, strict=False)
        distributed_model.mid_block1.load_state_dict(torch.load(Path(checkpoint_dir) / "unet_mid_block1.pth", map_location=distributed_model.device2))
        distributed_model.mid_attn.load_state_dict(torch.load(Path(checkpoint_dir) / "unet_mid_attn.pth", map_location=distributed_model.device2))
        distributed_model.mid_block2.load_state_dict(torch.load(Path(checkpoint_dir) / "unet_mid_block2.pth", map_location=distributed_model.device2))
        up_blocks_state_dict = torch.load(Path(checkpoint_dir) / "unet_up_blocks.pth")
        distributed_model.up_block_0.load_state_dict({k.replace('0.', ''): v for k, v in up_blocks_state_dict.items() if k.startswith('0.')}, strict=False)
        distributed_model.up_block_1.load_state_dict({k.replace('1.', ''): v for k, v in up_blocks_state_dict.items() if k.startswith('1.')}, strict=False)
        distributed_model.up_block_2.load_state_dict({k.replace('2.', ''): v for k, v in up_blocks_state_dict.items() if k.startswith('2.')}, strict=False)
        distributed_model.up_block_3.load_state_dict({k.replace('3.', ''): v for k, v in up_blocks_state_dict.items() if k.startswith('3.')}, strict=False)
        distributed_model.out_conv.load_state_dict(torch.load(Path(checkpoint_dir) / "unet_out_conv.pth", map_location=distributed_model.device0))
        model_for_inference = distributed_model # â˜… ä¿®æ­£: ãƒ­ãƒ¼ãƒ‰ã—ãŸåˆ†æ•£ãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ç”¨ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦è¨­å®š

    else: # gpu_mode == 'single'
        print(f"  Instantiating and loading models onto single device: {device}...")
        if encoder_name == 'resnet':
            conditioning_encoder = ConditioningEncoderResNet(output_dim=256)
        elif encoder_name == 'convnext':
            conditioning_encoder = ConditioningEncoderConvNeXt(output_dim=256)
        else: # efficientnet
            conditioning_encoder = ConditioningEncoderEfficientNetV2(output_dim=256)
        
        unet = DiffusionModelUNet(
            spatial_dims=3, in_channels=1, out_channels=1, with_conditioning=True,
            num_channels=(32, 64, 128, 256), attention_levels=(False, True, True, True),
            num_res_blocks=2, cross_attention_dim=conditioning_encoder.feature_dim
        )
        
        # å˜ä¸€ãƒ‡ãƒã‚¤ã‚¹ã«å…¨ãƒ‘ãƒ¼ãƒ„ã‚’ãƒ­ãƒ¼ãƒ‰
        conditioning_encoder.load_state_dict(torch.load(Path(checkpoint_dir) / "conditioning_encoder.pth", map_location=device))
        unet.time_mlp.load_state_dict(torch.load(Path(checkpoint_dir) / "unet_time_mlp.pth", map_location=device))
        unet.init_conv.load_state_dict(torch.load(Path(checkpoint_dir) / "unet_init_conv.pth", map_location=device))
        unet.down_blocks.load_state_dict(torch.load(Path(checkpoint_dir) / "unet_down_blocks.pth", map_location=device))
        unet.mid_block1.load_state_dict(torch.load(Path(checkpoint_dir) / "unet_mid_block1.pth", map_location=device))
        unet.mid_attn.load_state_dict(torch.load(Path(checkpoint_dir) / "unet_mid_attn.pth", map_location=device))
        unet.mid_block2.load_state_dict(torch.load(Path(checkpoint_dir) / "unet_mid_block2.pth", map_location=device))
        unet.out_conv.load_state_dict(torch.load(Path(checkpoint_dir) / "unet_out_conv.pth", map_location=device)) # â˜… ä¿®æ­£: out_convã®ãƒ­ãƒ¼ãƒ‰ã‚’è¿½åŠ 
        
        # ãƒ©ãƒƒãƒ‘ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«æ ¼ç´
        model_for_inference = {'unet': unet.to(device), 'conditioning_encoder': conditioning_encoder.to(device)}

    # 4. generate_and_evaluate é–¢æ•°ã®å‘¼ã³å‡ºã—
    print(f"\nCalling generate_and_evaluate for trial {trial_number}...")    
    try:
        generate_and_evaluate(
            device=device,
            params=params,
            scheduler_name="dpm_solver", # ãƒ†ã‚¹ãƒˆã—ãŸã„ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©å (dpm_solver, euler, ddpm)
            ct_full=ct_full,
            drr1=drr1,
            drr2=drr2,
            pos_3d=pos_3d,
            best_epoch=999, # ãƒ†ã‚¹ãƒˆãªã®ã§ãƒ€ãƒŸãƒ¼ã®å€¤ã§OK
            trial_number=trial_number,
            save_dir=checkpoint_dir,
            model_for_inference=model_for_inference
        )
        print("\nâœ… Generation and evaluation finished successfully.")
        print(f"   -> Check the output in: {checkpoint_dir}/evaluation/")
    except Exception as e:
        print(f"\nâŒ An error occurred during visualization: {e}")
        import traceback
        traceback.print_exc()
    finally:
        wandb.finish()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Run visualization using a trained model checkpoint.")
    parser.add_argument("--trial_number", type=int, required=True, 
                        help="The trial number whose checkpoint you want to test.")
    parser.add_argument("--encoder", type=str, required=True, choices=["resnet", "convnext", "efficientnet"], 
                        help="The encoder model used for that trial.")
    parser.add_argument("--gpu_mode", type=str, default="multi", choices=["single", "multi"],
                        help="GPU mode for inference: 'single' for one GPU, 'multi' for model parallelism across 3 GPUs.")
    parser.add_argument("--evaluate_npy", type=str, default=None,
                        help="Path to a generated .npy file to evaluate directly, skipping the inference step.")
    args = parser.parse_args()

    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’æ§‹ç¯‰ (train.pyã®ä¿å­˜ãƒ‘ã‚¹ã¨ä¸€è‡´ã•ã›ã‚‹)
    checkpoint_dir = f"./checkpoints/trial_{args.trial_number}"
    
    if not Path(checkpoint_dir).exists():
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {checkpoint_dir}")
        print("æ­£ã—ã„ trial_number ã‚’æŒ‡å®šã—ãŸã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    else:
        test_visualization(args.trial_number, checkpoint_dir, args.encoder, args.gpu_mode, args.evaluate_npy)
